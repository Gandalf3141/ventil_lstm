{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from icecream import ic\n",
    "from tqdm import tqdm\n",
    "from get_data import *\n",
    "from dataloader import *\n",
    "from test_function import test\n",
    "from NN_classes import *\n",
    "\n",
    "# Use the GPU if available\n",
    "torch.set_default_dtype(torch.float64)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "#device=\"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data generated via matlab/simulink:\n",
    "\n",
    "data_tensor, PSW_max = get_data(path = r\"data\\save_data_test5.csv\", timesteps_from_data=0, skip_steps_start = 0,\n",
    "                                 skip_steps_end = 0, drop_half_timesteps = False, normalise_s_w=None, rescale_p=False, num_inits=0)\n",
    "\n",
    "# View an example of a simulation run\n",
    "visualise(data_tensor, num_inits=9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "#path = f\"Ventil_trained_NNs\\lstm_ws0.pth\"\n",
    "#torch.save(model.state_dict(), path)\n",
    "\n",
    "# Load the model and test it on the test data\n",
    "path = \"Ventil_trained_NNs/OR_MLP0.pth\"\n",
    "\n",
    "params =                                                    {\n",
    "                            \"experiment_number\" : 2,\n",
    "                           \"window_size\" : 20,\n",
    "                           \"h_size\" : 25,\n",
    "                           \"l_num\" : 2,\n",
    "                           \"epochs\" : 500,\n",
    "                           \"learning_rate\" : 0.001,\n",
    "                           \"part_of_data\" : 100, \n",
    "                           \"weight_decay\" : 1e-5,\n",
    "                           \"percentage_of_data\" : 0.8,\n",
    "                           \"batch_size\" : 200,\n",
    "                           \"cut_off_timesteps\" : 0,\n",
    "                           \"drop_half_timesteps\": True,\n",
    "                           \"act_fn\" : \"relu\",\n",
    "                           \"nonlin_at_out\" : None #None if no nonlinearity at the end\n",
    "                        }\n",
    "\n",
    "# Generate input data (the data is normalized and some timesteps are cut off)\n",
    "input_data1, PSW_max = get_data(path = \"data/save_data_test_5xlonger.csv\", \n",
    "                        timesteps_from_data=0, \n",
    "                        skip_steps_start = 0,\n",
    "                        skip_steps_end = 0, \n",
    "                        drop_half_timesteps = params[\"drop_half_timesteps\"],\n",
    "                        normalise_s_w=\"minmax\",\n",
    "                        rescale_p=False,\n",
    "                        num_inits=params[\"part_of_data\"])\n",
    "\n",
    "input_data2, PSW_max = get_data(path = \"data/save_data_test_revised.csv\", \n",
    "                        timesteps_from_data=0, \n",
    "                        skip_steps_start = 0,\n",
    "                        skip_steps_end = 0, \n",
    "                        drop_half_timesteps = params[\"drop_half_timesteps\"],\n",
    "                        normalise_s_w=\"minmax\",\n",
    "                        rescale_p=False,\n",
    "                        num_inits=params[\"part_of_data\"])\n",
    "\n",
    "#input_data = torch.cat((input_data1, input_data2))\n",
    "input_data = input_data2\n",
    "print(input_data.size())\n",
    "\n",
    "#Split data into train and test sets\n",
    "np.random.seed(1234)\n",
    "num_of_inits_train = int(len(input_data)*params[\"percentage_of_data\"])\n",
    "train_inits = np.random.choice(np.arange(len(input_data)),num_of_inits_train,replace=False)\n",
    "test_inits = np.array([x for x in range(len(input_data)) if x not in train_inits])\n",
    "\n",
    "train_data = input_data[train_inits,:input_data.size(dim=1)-params[\"cut_off_timesteps\"],:]\n",
    "test_data = input_data[test_inits,:,:]\n",
    "np.random.seed()\n",
    "\n",
    "# dataloader for batching during training\n",
    "train_set = CustomDataset_mlp(train_data, window_size=params[\"window_size\"])\n",
    "train_loader = DataLoader(train_set, batch_size=params[\"batch_size\"])#, pin_memory=True)\n",
    "\n",
    "\n",
    "model = OR_MLP(input_size=3*params[\"window_size\"], hidden_size = params[\"h_size\"], l_num=params[\"l_num\"], output_size=2, act_fn = params[\"act_fn\"],\n",
    "               timesteps=params[\"window_size\"], act_at_end = params[\"nonlin_at_out\"]).to(device)\n",
    "model.load_state_dict(torch.load(path, map_location=torch.device(device)))\n",
    "                                 \n",
    "train_data = input_data[train_inits,:,:]\n",
    "\n",
    "#test_loss, test_loss_deriv, total_loss = test(test_data, model, steps=input_data.size(dim=1), ws=params[\"window_size\"], plot_opt=True , n = 8, test_inits=len(test_data), rand=False, PSW_max=PSW_max)\n",
    "\n",
    "test_loss, test_loss_deriv, total_loss = test(input_data2.to(device), model, model_type = \"or_mlp\", window_size=params[\"window_size\"],\n",
    "                                               display_plots=True, num_of_inits = 10, set_rand_seed=False, physics_rescaling = PSW_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                        #           {\n",
    "                        #     \"experiment_number\" : 2,\n",
    "                        #    \"window_size\" : 5,\n",
    "                        #    \"h_size\" : 8,\n",
    "                        #    \"l_num\" : 1,\n",
    "                        #    \"epochs\" : 500,\n",
    "                        #    \"learning_rate\" : 0.001,\n",
    "                        #    \"part_of_data\" : 100, \n",
    "                        #    \"weight_decay\" : 1e-5,\n",
    "                        #    \"percentage_of_data\" : 0.8,\n",
    "                        #    \"batch_size\" : 200,\n",
    "                        #    \"cut_off_timesteps\" : 0,\n",
    "                        #    \"drop_half_timesteps\": True,\n",
    "                        #    \"act_fn\" : \"relu\",\n",
    "                        #    \"nonlin_at_out\" : None #None if no nonlinearity at the end\n",
    "                        # }\n",
    "\n",
    "import torch\n",
    "from torchviz import make_dot\n",
    "from IPython.display import Image\n",
    "\n",
    "x = next(iter(train_loader))[0]\n",
    "y = model(x)\n",
    "print(input_data2[0:1].size())\n",
    "dot = make_dot(y, params=dict(model.named_parameters()),  show_attrs=True, show_saved=True)   # .render(\"model_graph\", format=\"png\")\n",
    "\n",
    "#print(list(model.parameters()))\n",
    "\n",
    "#calc \n",
    "erg=0\n",
    "for a in list(model.parameters()):\n",
    "    x = 1 \n",
    "    for y in list(a.size()):\n",
    "        x = x*y\n",
    "    erg += x\n",
    "\n",
    "print(\"anzahl der variablen\" , erg)\n",
    "# Remove autograd related nodes\n",
    "#dot.attr(rankdir='LR')\n",
    "dot.node_attr.update(style='filled')\n",
    "\n",
    "# for node in dot.body:\n",
    "#     if 'Backward' in node:\n",
    "#         dot.body.remove(node)\n",
    "\n",
    "\n",
    "dot\n",
    "# Save the graph to a file\n",
    "# dot.format = 'png'\n",
    "# dot.render('model_graph')\n",
    "\n",
    "# Display the graph in the Jupyter notebook\n",
    "#Image(filename='model_graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer perceptron\n",
    "class OR_MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size=3, hidden_size = 6, l_num=1, output_size=2, act_fn=\"tanh\", act_at_end = None, timesteps=5):\n",
    "        super(OR_MLP, self).__init__()\n",
    "        \n",
    "        if act_fn == \"tanh\":\n",
    "            fn = nn.Tanh()\n",
    "        else:\n",
    "            fn = nn.ReLU()\n",
    "\n",
    "        hidden_sizes = [hidden_size for x in range(l_num)]\n",
    "        # Create a list to hold the layers\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        layers.append(fn)\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            layers.append(nn.Linear(hidden_sizes[i-1], hidden_sizes[i]))\n",
    "            layers.append(fn)\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        \n",
    "        #Try final non linearity:\n",
    "        if act_at_end != None:\n",
    "            if act_at_end == \"tanh\":\n",
    "                layers.append(nn.Tanh())\n",
    "            if act_at_end == \"relu\":\n",
    "                layers.append(nn.ReLU())\n",
    "            if act_at_end == \"sigmoid\":\n",
    "                layers.append(nn.Sigmoid())\n",
    "        \n",
    "        # Use nn.Sequential to put together the layers\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.ws = timesteps\n",
    "        #self.timesteps = timesteps\n",
    "    \n",
    "    def forward(self, one_full_traj):\n",
    "        \n",
    "        seq = one_full_traj[:, 0:self.ws, :]\n",
    "\n",
    "        #inp = torch.cat((seq[:, :self.ws,0], seq[:, :self.ws,1], seq[:, :self.ws,2]), dim=2)\n",
    "        inp = torch.stack([torch.cat((a[:, 0], a[:, 1], a[:, 2])) for a in seq])\n",
    "        pred = self.network(inp) \n",
    "        \n",
    "        out = one_full_traj[:, self.ws-1:self.ws, 1:] + pred.view(one_full_traj.size(dim=0),1,2)\n",
    "        print(\"input\", inp)\n",
    "        print(\"pred\", pred)\n",
    "        print(\"outout\", out)\n",
    "\n",
    "        for t in range(1, self.ws): # f√ºr RK : range(1, self.ws + 2):\n",
    "\n",
    "\n",
    "            tmp = torch.cat((one_full_traj[:,self.ws+(t-1):self.ws+(t-1)+(out.size(dim=1)), 0:1] , out[:, :, :]), dim=2)\n",
    "            seq = torch.cat((one_full_traj[:, t:self.ws, :], tmp), dim=1)\n",
    "\n",
    "            inp = torch.stack([torch.cat((a[:, 0], a[:, 1], a[:, 2])) for a in seq])\n",
    "\n",
    "            pred = self.network(inp)\n",
    "\n",
    "            out = torch.cat((out, out[:, -1:, 1:] + pred.view(one_full_traj.size(dim=0),1,2)), dim=1)\n",
    "            print(\"input\", inp)\n",
    "            print(\"pred\", pred)\n",
    "            print(\"outout\", out)\n",
    "\n",
    "        for t in range(self.ws, one_full_traj.size(dim=1) - self.ws):\n",
    "\n",
    "            seq = torch.cat((one_full_traj[:, t : t + self.ws, 0:1], out[:, t - self.ws : t , :]), dim=2)\n",
    "            \n",
    "            inp = torch.stack([torch.cat((a[:, 0], a[:, 1], a[:, 2])) for a in seq])\n",
    "\n",
    "            pred = self.network(inp)\n",
    "\n",
    "            out = torch.cat((out, out[:, -1:, 1:] + pred.view(one_full_traj.size(dim=0),1,2)), dim=1)\n",
    "            print(\"input\", inp)\n",
    "            print(\"pred\", pred)\n",
    "            print(\"outout\", out)\n",
    "        return out        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "#path = f\"Ventil_trained_NNs\\lstm_ws0.pth\"\n",
    "#torch.save(model.state_dict(), path)\n",
    "\n",
    "\n",
    "params =                                                    {\n",
    "                            \"experiment_number\" : 2,\n",
    "                           \"window_size\" : 5,\n",
    "                           \"h_size\" : 8,\n",
    "                           \"l_num\" : 1,\n",
    "                           \"epochs\" : 500,\n",
    "                           \"learning_rate\" : 0.001,\n",
    "                           \"part_of_data\" : 100, \n",
    "                           \"weight_decay\" : 1e-5,\n",
    "                           \"percentage_of_data\" : 0.8,\n",
    "                           \"batch_size\" : 200,\n",
    "                           \"cut_off_timesteps\" : 0,\n",
    "                           \"drop_half_timesteps\": True,\n",
    "                           \"act_fn\" : \"relu\",\n",
    "                           \"nonlin_at_out\" : None #None if no nonlinearity at the end\n",
    "                        }\n",
    "\n",
    "# Generate input data (the data is normalized and some timesteps are cut off)\n",
    "input_data1, PSW_max = get_data(path = \"data\\save_data_test_5xlonger.csv\", \n",
    "                        timesteps_from_data=0, \n",
    "                        skip_steps_start = 0,\n",
    "                        skip_steps_end = 0, \n",
    "                        drop_half_timesteps = params[\"drop_half_timesteps\"],\n",
    "                        normalise_s_w=\"minmax\",\n",
    "                        rescale_p=False,\n",
    "                        num_inits=params[\"part_of_data\"])\n",
    "\n",
    "\n",
    "input_data = input_data1\n",
    "print(input_data.size())\n",
    "\n",
    "#Split data into train and test sets\n",
    "np.random.seed(1234)\n",
    "num_of_inits_train = int(len(input_data)*params[\"percentage_of_data\"])\n",
    "train_inits = np.random.choice(np.arange(len(input_data)),num_of_inits_train,replace=False)\n",
    "test_inits = np.array([x for x in range(len(input_data)) if x not in train_inits])\n",
    "\n",
    "train_data = input_data[train_inits,:input_data.size(dim=1)-params[\"cut_off_timesteps\"],:]\n",
    "test_data = input_data[test_inits,:,:]\n",
    "np.random.seed()\n",
    "\n",
    "# dataloader for batching during training\n",
    "train_set = CustomDataset_mlp(train_data, window_size=params[\"window_size\"])\n",
    "train_loader = DataLoader(train_set, batch_size=params[\"batch_size\"])#, pin_memory=True)\n",
    "\n",
    "\n",
    "model = OR_MLP(input_size=3*params[\"window_size\"], hidden_size = params[\"h_size\"], l_num=params[\"l_num\"], output_size=2, act_fn = params[\"act_fn\"], act_at_end = params[\"nonlin_at_out\"]).to(device)\n",
    "                              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_new = input_data[0:3,0:15:,:].clone()\n",
    "input_data_new[0:1,:,0:1] = 1\n",
    "input_data_new[0:1,:,1:2] = 2\n",
    "input_data_new[0:1,:,2:] = 3\n",
    "input_data_new[1:2,:,0:1] = 4\n",
    "input_data_new[1:2,:,1:2] = 5\n",
    "input_data_new[1:2,:,2:] = 6\n",
    "input_data_new[2:,:,0:1] = 7\n",
    "input_data_new[2:,:,1:2] = 8\n",
    "input_data_new[2:,:,2:] = 9\n",
    "\n",
    "model(input_data_new)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
