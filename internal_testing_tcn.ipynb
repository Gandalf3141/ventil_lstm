{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import weight_norm\n",
    "from get_data import *\n",
    "from dataloader import *\n",
    "from test_function import *\n",
    "from NN_classes import TCN\n",
    "import logging\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# path = f\"Ventil_trained_NNs\\lstm_ws0.pth\"\n",
    "# #\n",
    "# torch.save(model.state_dict(), path)\n",
    "\n",
    "# Load the model and test it on the test data\n",
    "path = r\"Ventil_trained_NNs\\tcn_0.pth\"\n",
    "\n",
    "params =                          {\n",
    "                            \"experiment_number\" : 4,\n",
    "                            \"window_size\" : 50,\n",
    "                            \"epochs\" : 50,\n",
    "                            \"learning_rate\" : 0.001,\n",
    "                            \"part_of_data\" : 10, \n",
    "                            \"percentage_of_data\" : 0.7,\n",
    "                            \"batch_size\" : 20,\n",
    "                            \"future\" : 1,\n",
    "                            \"drop_half_timesteps\" : True,\n",
    "                            \"cut_off_timesteps\" : 0,\n",
    "\n",
    "                            \"input_channels\" : 3,\n",
    "                            \"output\" : 2,\n",
    "                            \"n_hidden\" : 5,\n",
    "                            \"levels\" : 4,\n",
    "                            \"kernel_size\" : 5,\n",
    "                            \"dropout\" : 0\n",
    "\n",
    "                        }\n",
    "\n",
    "\n",
    "\n",
    "input_data1, PSW_max = get_data(path = \"data\\save_data_test_revised.csv\", \n",
    "                        timesteps_from_data=0, \n",
    "                        skip_steps_start = 0,\n",
    "                        skip_steps_end = 0, \n",
    "                        drop_half_timesteps = params[\"drop_half_timesteps\"],\n",
    "                        normalise_s_w=\"minmax\",\n",
    "                        rescale_p=False,\n",
    "                        num_inits=params[\"part_of_data\"])\n",
    "\n",
    "input_data2, PSW_max = get_data(path = \"data\\save_data_test5.csv\", \n",
    "                        timesteps_from_data=0, \n",
    "                        skip_steps_start = 0,\n",
    "                        skip_steps_end = 0, \n",
    "                        drop_half_timesteps = params[\"drop_half_timesteps\"],\n",
    "                        normalise_s_w=\"minmax\",\n",
    "                        rescale_p=False,\n",
    "                        num_inits=params[\"part_of_data\"])\n",
    "\n",
    "input_data3, PSW_max = get_data(path = \"data\\Testruns_from_trajectory_generator_t2_t6_revised.csv\", \n",
    "                        timesteps_from_data=0, \n",
    "                        skip_steps_start = 0,\n",
    "                        skip_steps_end = 0, \n",
    "                        drop_half_timesteps = params[\"drop_half_timesteps\"],\n",
    "                        normalise_s_w=\"minmax\",\n",
    "                        rescale_p=False,\n",
    "                        num_inits=params[\"part_of_data\"])\n",
    "                        \n",
    "\n",
    "input_data = torch.cat((input_data1, input_data2, input_data3))\n",
    "input_data=input_data1\n",
    "\n",
    "np.random.seed(1234)\n",
    "print(\"input_data size\", input_data.size())\n",
    "num_of_inits_train = int(len(input_data)*params[\"percentage_of_data\"])\n",
    "train_inits = np.random.choice(np.arange(len(input_data)),num_of_inits_train,replace=False)\n",
    "test_inits = np.array([x for x in range(len(input_data)) if x not in train_inits])\n",
    "np.random.shuffle(train_inits)\n",
    "np.random.shuffle(test_inits)\n",
    "test_data = input_data[test_inits,:,:]\n",
    "np.random.seed()\n",
    "\n",
    "# Initialize the LSTM model\n",
    "\n",
    "input_channels = params[\"input_channels\"]\n",
    "output = params[\"output\"]\n",
    "num_channels = [params[\"n_hidden\"]] * params[\"levels\"]\n",
    "kernel_size = params[\"kernel_size\"]\n",
    "dropout = params[\"dropout\"]\n",
    "\n",
    "model = TCN(input_channels, output, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "\n",
    "model.load_state_dict(torch.load(path, map_location=torch.device(device)))\n",
    "\n",
    "train_data = input_data[train_inits,:,:]\n",
    "#%matplotlib qt \n",
    "#%matplotlib inline \n",
    "\n",
    "#test_loss, test_loss_deriv, total_loss, physloss\n",
    "test_loss, test_loss_deriv, total_loss = test(train_data, model, model_type = \"tcn\", window_size=params[\"window_size\"],\n",
    "                                                         display_plots=True, num_of_inits = 3, set_rand_seed=False, physics_rescaling = PSW_max, additional_data=None)\n",
    "#test_loss, test_loss_deriv, total_loss = test(train_data, model, steps=input_data.size(dim=1), ws=params[\"window_size\"], plot_opt=True , n = 1,  test_inits=len(test_data), rand=False, PSW_max=0)\n",
    "\n",
    "#[350000.0, 0.0006, 1.7, 0.0, 0.0, -1.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "       # self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "       # self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        # self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "        #                          self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1,\n",
    "                                 self.conv2, self.chomp2, self.relu2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
    "        super(TCN, self).__init__()\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.linear.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.tcn(x)\n",
    "        return self.linear(y1[:, :, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = r\"Ventil_trained_NNs\\tcn_0.pth\"\n",
    "\n",
    "params =                 {\n",
    "                            \"window_size\" : 50,\n",
    "                            \"learning_rate\" : 0.001,\n",
    "                            \"part_of_data\" : 10, \n",
    "                            \"percentage_of_data\" : 0.7,\n",
    "                            \"batch_size\" : 20,\n",
    "                            \"drop_half_timesteps\" : True,\n",
    "                            \"cut_off_timesteps\" : 0,\n",
    "\n",
    "                            \"input_channels\" : 3,\n",
    "                            \"output\" : 2,\n",
    "                            \"n_hidden\" : 5,\n",
    "                            \"levels\" : 1,\n",
    "                            \"kernel_size\" : 7,\n",
    "                            \"dropout\" : 0\n",
    "\n",
    "                        }\n",
    "\n",
    "\n",
    "\n",
    "input_data1, PSW_max = get_data(path = \"data\\save_data_test_revised.csv\", \n",
    "                        timesteps_from_data=0, \n",
    "                        skip_steps_start = 0,\n",
    "                        skip_steps_end = 0, \n",
    "                        drop_half_timesteps = params[\"drop_half_timesteps\"],\n",
    "                        normalise_s_w=\"minmax\",\n",
    "                        rescale_p=False,\n",
    "                        num_inits=params[\"part_of_data\"])\n",
    "\n",
    "input_data=input_data1\n",
    "\n",
    "np.random.seed(1234)\n",
    "print(\"input_data size\", input_data.size())\n",
    "num_of_inits_train = int(len(input_data)*params[\"percentage_of_data\"])\n",
    "train_inits = np.random.choice(np.arange(len(input_data)),num_of_inits_train,replace=False)\n",
    "test_inits = np.array([x for x in range(len(input_data)) if x not in train_inits])\n",
    "np.random.shuffle(train_inits)\n",
    "np.random.shuffle(test_inits)\n",
    "test_data = input_data[test_inits,:,:]\n",
    "np.random.seed()\n",
    "train_data = input_data[train_inits,:,:]\n",
    "\n",
    "\n",
    "input_channels = params[\"input_channels\"]\n",
    "output = params[\"output\"]\n",
    "num_channels = [params[\"n_hidden\"]] * params[\"levels\"]\n",
    "kernel_size = params[\"kernel_size\"]\n",
    "dropout = params[\"dropout\"]\n",
    "\n",
    "model = TCN(input_channels, output, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "from IPython.display import Image\n",
    "\n",
    "y = model(input_data1[0:1].transpose(1,2))\n",
    "print(input_data1[0:1].size())\n",
    "dot = make_dot(y, params=dict(model.named_parameters()),  show_attrs=True, show_saved=True)   # .render(\"model_graph\", format=\"png\")\n",
    "\n",
    "#print(list(model.parameters()))\n",
    "\n",
    "#calc \n",
    "erg=0\n",
    "for a in list(model.parameters()):\n",
    "    x = 1 \n",
    "    for y in list(a.size()):\n",
    "        x = x*y\n",
    "    erg += x\n",
    "\n",
    "print(\"anzahl der variablen\" , erg)\n",
    "# Remove autograd related nodes\n",
    "#dot.attr(rankdir='LR')\n",
    "dot.node_attr.update(style='filled')\n",
    "\n",
    "# for node in dot.body:\n",
    "#     if 'Backward' in node:\n",
    "#         dot.body.remove(node)\n",
    "\n",
    "\n",
    "dot\n",
    "# Save the graph to a file\n",
    "# dot.format = 'png'\n",
    "# dot.render('model_graph')\n",
    "\n",
    "# Display the graph in the Jupyter notebook\n",
    "#Image(filename='model_graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
