{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from matplotlib import legend\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import numpy as np\n",
    "from icecream import ic\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from get_data import get_data\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Define the LSTM model with two hidden layers\n",
    "torch.set_default_dtype(torch.float64)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "class LSTMmodel(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM model class for derivative estimation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, out_size, layers):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM model.\n",
    "\n",
    "        Args:\n",
    "        - input_size: Size of input\n",
    "        - hidden_size: Size of hidden layer\n",
    "        - out_size: Size of output\n",
    "        - layers: Number of layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # Define LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=layers)\n",
    "\n",
    "        # Define linear layer\n",
    "        self.linear = nn.Linear(hidden_size, out_size)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        \"\"\"\n",
    "        Forward pass through the LSTM model.\n",
    "\n",
    "        Args:\n",
    "        - seq: Input sequence\n",
    "\n",
    "        Returns:\n",
    "        - pred: Model prediction\n",
    "        - hidden: Hidden state\n",
    "        \"\"\"\n",
    "        lstm_out, hidden = self.lstm(seq)\n",
    "        pred = self.linear(lstm_out.view(len(seq), -1))\n",
    "\n",
    "        return pred, hidden\n",
    "\n",
    "\n",
    "def slice_batch(batch, window_size=1):\n",
    "    \"\"\"\n",
    "    Slice the input data into batches for training.\n",
    "\n",
    "    Args:\n",
    "    - batch: Input data batch\n",
    "    - window_size: Size of the sliding window\n",
    "\n",
    "    Returns:\n",
    "    - List of sliced batches\n",
    "    \"\"\"\n",
    "    l = []\n",
    "    for i in range(len(batch) - window_size):\n",
    "        l.append((batch[i:i+window_size, :], batch[i+1:i+window_size+1, 1:]))\n",
    "    return l\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(input_data, model, ws=1):\n",
    "    \"\"\"\n",
    "    Train the LSTM model using input data.\n",
    "\n",
    "    Args:\n",
    "    - input_data: Input data for training\n",
    "    - model: LSTM model to be trained\n",
    "    - ws: Window size\n",
    "    - odestep: Option for using ODE steps\n",
    "    - use_autograd: Option for using autograd\n",
    "\n",
    "    Returns:\n",
    "    - Mean loss over all batches\n",
    "    \"\"\"\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    total_loss = []\n",
    "\n",
    "    for batch in input_data:\n",
    "\n",
    "        input = slice_batch(batch, ws)\n",
    "        batch_loss = 0\n",
    "\n",
    "        for inp, label in input:  # inp = (u, x) label = x\n",
    "\n",
    "            output, _ = model(inp)\n",
    "\n",
    "            #reconsider this part :\n",
    "            #maybe | out = inp[-1, 1:] + output[-1] | works better\n",
    "            #out = inp[:, 1:] + output\n",
    "            out = inp[-1, 1:] + output[-1]\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss = loss_fn(out, label[-1])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_loss += loss.detach().cpu().numpy()\n",
    "\n",
    "        total_loss.append(batch_loss/len(batch))\n",
    "\n",
    "    return np.mean(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(test_data, model, steps=600, ws=10):\n",
    "\n",
    "    model.eval()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    test_loss = 0\n",
    "    test_loss_deriv = 0\n",
    "\n",
    "    for i, x in enumerate(test_data):\n",
    "        \n",
    "        if i > 3:\n",
    "            break\n",
    "\n",
    "        with torch.inference_mode():\n",
    "\n",
    "            pred = torch.zeros((steps, 3), device=device)\n",
    "            pred_next_step = torch.zeros((steps, 3), device=device)\n",
    "\n",
    "            if ws > 1:\n",
    "                pred[0:ws, :] = x[0:ws, :]\n",
    "                pred[:, 0] = x[:, 0]\n",
    "                pred_next_step[0:ws, :] = x[0:ws, :]\n",
    "                pred_next_step[:, 0] = x[:, 0]\n",
    "            else:\n",
    "                pred[0, :] = x[0, :]\n",
    "                pred[:, 0] = x[:, 0]\n",
    "                pred_next_step[0, :] = x[0, :]\n",
    "                pred_next_step[:, 0] = x[:, 0]\n",
    "\n",
    "            for i in range(len(x) - ws):\n",
    "                out, _ = model(pred[i:i+ws, :])\n",
    "\n",
    "                pred[i+ws, 1:] = pred[i+ws-1, 1:] + out[-1, :]\n",
    "                pred_next_step[i+ws, 1:] = x[i+ws-1, 1:] + out[-1, :]\n",
    "            \n",
    "            test_loss += loss_fn(pred[:, 1], x[:, 1]).detach().cpu().numpy()\n",
    "            test_loss_deriv += loss_fn(pred[:, 2], x[:, 2]).detach().cpu().numpy()\n",
    "\n",
    "            figure , axs = plt.subplots(1,3,figsize=(16,9))\n",
    "            \n",
    "            axs[0].plot(np.linspace(0,1,steps), pred.detach().cpu().numpy()[:, 1], color=\"red\", label=\"pred\")\n",
    "            axs[0].plot(np.linspace(0,1,steps), pred_next_step.detach().cpu().numpy()[:, 1], color=\"green\", label=\"next step from data\")\n",
    "            axs[0].plot(np.linspace(0,1,steps), x.detach().cpu().numpy()[:, 1], color=\"blue\", label=\"true\", linestyle=\"dashed\")\n",
    "\n",
    "            axs[0].grid()\n",
    "            axs[0].legend()\n",
    "\n",
    "            axs[1].plot(np.linspace(0,1,steps), pred.detach().cpu().numpy()[\n",
    "            :, 2], color=\"red\", label=\"pred\")\n",
    "            axs[1].plot(np.linspace(0,1,steps), pred_next_step.detach().cpu().numpy()[:, 2], color=\"green\", label=\"next step from data\")\n",
    "            axs[1].plot(np.linspace(0,1,steps), x.detach().cpu().numpy()[:, 2], color=\"blue\", label=\"true\", linestyle=\"dashed\")\n",
    "\n",
    "            axs[1].grid()\n",
    "            axs[1].legend()\n",
    "            axs[2].plot(np.linspace(0,1,steps), x.detach().cpu().numpy()[:,0], label=\"pressure\")\n",
    "\n",
    "            axs[2].grid()\n",
    "            axs[2].legend()\n",
    "            \n",
    "    return np.mean(test_loss), np.mean(test_loss_deriv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "\n",
    "    log_file = 'training.log'\n",
    "    filemode = 'a' if os.path.exists(log_file) else 'w'\n",
    "\n",
    "    # Configure logging\n",
    "    logging.basicConfig(filename=log_file, filemode=filemode, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    # Define parameters\n",
    "    window_size = 4\n",
    "\n",
    "    losses = []\n",
    "    device=\"cpu\"\n",
    "    # Generate input data\n",
    "    input_data = get_data(path=\"save_data_test.csv\")\n",
    "    \n",
    "    input_data = input_data.to(device)\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    train_size = int(0.5 * len(input_data))\n",
    "    test_size = len(input_data) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(input_data, [train_size, test_size])\n",
    "\n",
    "    # Take a slice of data for training (only slice_of_data many timesteps)\n",
    "    slice_of_data = 20\n",
    "\n",
    "    train_dataset = train_dataset[:][:, 0:slice_of_data, :]\n",
    "\n",
    "    # Initialize the LSTM model\n",
    "    model = LSTMmodel(input_size=3, hidden_size=5, out_size=2, layers=1).to(device)\n",
    "\n",
    "    trained=False\n",
    "    if trained:\n",
    "     path = f\"Ventil_trained_NNs\\lstm_ws{window_size}.pth\"\n",
    "     \n",
    "     model.load_state_dict(torch.load(path, map_location=torch.device(device)))\n",
    "    else:\n",
    "        model = LSTMmodel(input_size=3, hidden_size=5, out_size=2, layers=1).to(device)\n",
    "    \n",
    "    #Train\n",
    "    epochs = 100\n",
    "    for e in tqdm(range(epochs)):\n",
    "        loss_epoch = train(train_dataset, model, ws=window_size)\n",
    "        losses.append(loss_epoch)\n",
    "        if e % 2 == 0 or 1:\n",
    "            print(f\"Epoch {e}: Loss: {loss_epoch}\")\n",
    "   \n",
    "    # Plot losses\n",
    "    plt.plot(losses[1:])\n",
    "    plt.show()\n",
    "\n",
    "    # Save trained model\n",
    "    path = f\"Ventil_trained_NNs\\lstm_ws{window_size}.pth\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "    #test the model\n",
    "\n",
    "    a,b=test(test_dataset, model, steps=600, ws=10)\n",
    "    print(\"s\", a, \"w\", b)\n",
    "\n",
    "    # Log parameters\n",
    "    logging.info(f\"Epochs: {epochs}, Window Size: {window_size}\")\n",
    "    logging.info(f\"final loss {losses[-1]}\")\n",
    "    logging.info(\"--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "    logging.info(\"--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "    logging.info(\"\\n\")\n",
    "    logging.info(\"\\n\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# chat gpt instructions\n",
    "# Add short and precise comments to the following python code. Describe functions, classes, loops similar things. The code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data(path = \"ventil_lstm\\save_data_test.csv\"):\n",
    "    \n",
    "    df = pd.read_csv(path, header=0, nrows=600, skiprows=[x for x in range(1,100)])\n",
    "\n",
    "    #drop even more timesteps\n",
    "    df = df.iloc[::2]\n",
    "\n",
    "    #Reorder columns for familiar setup (t,u,x) here (t, p_b, s_b, w_b)\n",
    "    L = df.columns.to_list()\n",
    "    time_cols = L[0::4]\n",
    "    sb_cols = L[1::4]\n",
    "    pb_cols = L[2::4]\n",
    "    wb_cols = L[3::4]\n",
    "    new_col_order = [x for sub in list(zip(time_cols, pb_cols, sb_cols, wb_cols)) for x in sub]\n",
    "    df= df[new_col_order]\n",
    "    df = df.drop(time_cols, axis=1)\n",
    "\n",
    "    #normalise each column of the dataframe\n",
    "    #mean normalization\n",
    "    #df=(df-df.mean())/df.std()\n",
    "\n",
    "    #min max normalization\n",
    "    #normalize only a part of the data(??)\n",
    "    df[sb_cols+wb_cols]=(df[sb_cols+wb_cols]-df[sb_cols+wb_cols].min())/(df[sb_cols+wb_cols].max()-df[sb_cols+wb_cols].min())\n",
    "    \n",
    "    #Can't normalize p_b because then a[i]*X+b[i] becomes cX+d for all i.. same with mean normal. \n",
    "    \n",
    "    df[pb_cols] = df[pb_cols] / 1e5\n",
    "\n",
    "    tensor = torch.tensor(df.values)\n",
    "\n",
    "    #tensor with t=0:600, 500 different input and the 4 outputs [time, s_b, p_b, w_b]\n",
    "    tensor = tensor.view(len(df),500,3).permute(1,0,2)\n",
    "\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = get_data(path=\"save_data_test.csv\")\n",
    "input_data=input_data.to(device)\n",
    "# Split data into train and test sets\n",
    "train_size = int(0.7 * len(input_data))\n",
    "test_size = len(input_data) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(input_data, [train_size, test_size])\n",
    "\n",
    "# Take a slice of data for training (only slice_of_data many timesteps)\n",
    "slice_of_data = 60\n",
    "\n",
    "train_dataset = train_dataset[:][:, 0:slice_of_data, :]\n",
    "\n",
    "model = LSTMmodel(input_size=3, hidden_size=5, out_size=2, layers=1).to(device)\n",
    "\n",
    "window_size = 16\n",
    "h_size=8\n",
    "l_num=2\n",
    "model = LSTMmodel(input_size=3, hidden_size=h_size, out_size=2, layers=l_num).to(device)\n",
    "\n",
    "path = f\"Ventil_trained_NNs\\lstm_ws{window_size}hs{h_size}layer{l_num}.pth\"\n",
    "\n",
    "model.load_state_dict(torch.load(path, map_location=torch.device(device)))\n",
    "a,b=test(test_dataset, model, steps=300, ws=4)\n",
    "print(\"s\", a, \"w\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def get_data2(path):\n",
    "    return torch.tensor([])\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, path=\"-\", seq_len=1):\n",
    "        self.data_tensor = get_data2(path) \n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(data_tensor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.randint(low=0, high=10, size=(3, 8, 3))\n",
    "print(tensor)\n",
    "\n",
    "ws=2\n",
    "\n",
    "def get(id,tensor,ws):\n",
    "\n",
    "    j = int(np.floor((id+ws) / tensor.size(1)))\n",
    "    print(\"id\", id, \"j\", j)\n",
    "    if np.floor((id) / tensor.size(1))<np.floor((id+ws) / tensor.size(1)):\n",
    "\n",
    "        off = id+(ws) - j*tensor.size(1)\n",
    "        \n",
    "        id = off \n",
    "\n",
    "        print(off, id+ws)\n",
    "\n",
    "        inp = tensor[j,id:id+ws,:]\n",
    "        label = tensor[j,id+ws,:]\n",
    "\n",
    "        return inp, label\n",
    "    \n",
    "    else: \n",
    "        \n",
    "        inp = tensor[j,id:id+ws,:]\n",
    "        label = tensor[j,id+ws,:]\n",
    "\n",
    "        return inp, label\n",
    "\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    a, b  =get(i, tensor, 3)\n",
    "    print(a)\n",
    "   # print(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
