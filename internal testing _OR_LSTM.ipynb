{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from icecream import ic\n",
    "from tqdm import tqdm\n",
    "from get_data import *\n",
    "from dataloader import *\n",
    "from test_function import test"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 41,
>>>>>>> 56617d4d212940381c717bbfd9bee16f3ba39408
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class custom_simple_dataset(Dataset):\n",
    "\n",
    "\n",
    "    def __init__(self, data, window_size):\n",
    "\n",
    "        self.data = data\n",
    "        self.ws = window_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        inp = self.data[idx, :, :]\n",
    "        label = self.data[idx, self.ws:, 1:]\n",
    "\n",
    "        return inp, label\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 45,
>>>>>>> 291077b3698dc9667d8383b80f68e8b1ec597541
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "cuda:0\n"
=======
      "cpu\n"
>>>>>>> 291077b3698dc9667d8383b80f68e8b1ec597541
     ]
    }
   ],
>>>>>>> 56617d4d212940381c717bbfd9bee16f3ba39408
   "source": [
    " #Define the LSTM model class\n",
    "\n",
    "# Use the GPU if available\n",
    "torch.set_default_dtype(torch.float64)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "#device=\"cpu\"\n",
    "print(device)\n",
    "\n",
    "RK = True\n",
    "\n",
    "class LSTMmodel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, out_size, layers, window_size=4, stepsize=1, rungekutta=False):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.ws = window_size\n",
    "        self.rungekutta = rungekutta\n",
    "        if stepsize==1:\n",
    "         self.step_size = 1\n",
    "        else:\n",
    "         self.step_size = torch.nn.parameter.Parameter(torch.rand(1))\n",
    "\n",
    "        # Define LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=layers, batch_first=True)\n",
    "\n",
    "        # Define linear layer\n",
    "        self.linear = nn.Linear(hidden_size, out_size)\n",
    "\n",
    "    def forward(self, one_full_traj):\n",
    "\n",
    "        seq = one_full_traj[:, 0:self.ws, :]\n",
    "        lstm_out, hidden = self.lstm(seq)           \n",
    "        pred = self.linear(lstm_out)\n",
    "        #only update next step\n",
    "        out = one_full_traj[:, self.ws-1:self.ws, 1:] + pred[:, -1: , :]\n",
    "        #out = one_full_traj[:, self.ws-1:self.ws, 1:] + pred\n",
    "\n",
    "        if RK != self.rungekutta:\n",
    "            for t in range(1, self.ws): # f체r RK : range(1, self.ws + 2):\n",
    "\n",
<<<<<<< HEAD
    "                tmp = torch.cat(( one_full_traj[:,self.ws+t:self.ws+t+1, 0:1] , out[:, (t-1):t,:]), dim=2)\n",
    "                seq = torch.cat((one_full_traj[:, t:self.ws+(t-1), :], tmp), dim=1)\n",
    "                lstm_out, hidden = self.lstm(seq)           \n",
    "                pred = self.linear(lstm_out)\n",
    "                out = torch.cat((out, one_full_traj[:, self.ws+(t-1): self.ws+t, 1:] + pred[:, -1: , :]), dim=1)\n",
=======
    "            tmp = torch.cat(( one_full_traj[:,self.ws+t:self.ws+t+1, 0:1] , out[:, (t-1):t,:]), dim=2)\n",
    "            seq = torch.cat((one_full_traj[:, t:self.ws+(t-1), :], tmp), dim=1)\n",
    "            lstm_out, hidden = self.lstm(seq)           \n",
    "            pred = self.linear(lstm_out)\n",
<<<<<<< HEAD
    "            #only update next step\n",
    "            out = torch.cat((out, one_full_traj[:, self.ws+(t-1): self.ws+t, 1:] +  pred[:, -1: , :]), dim=1)\n",
    "           # out = torch.cat((out, one_full_traj[:, self.ws+(t-1): self.ws+t, 1:] +  pred), dim=1)\n",
=======
    "            out = torch.cat((out, one_full_traj[:, self.ws+(t-1): self.ws+t, 1:] + pred[:, -1: , :]), dim=1)\n",
>>>>>>> 291077b3698dc9667d8383b80f68e8b1ec597541
>>>>>>> 56617d4d212940381c717bbfd9bee16f3ba39408
    "\n",
    "            for t in range(self.ws, one_full_traj.size(dim=1) - self.ws):\n",
    "\n",
    "                #w채re richtig!!!    \n",
    "                seq = torch.cat((one_full_traj[:, t : t + self.ws, 0:1], out[:, t - self.ws : t , :]), dim=2)\n",
    "                #war falsch\n",
    "                #seq = torch.cat((out[:, t - self.ws : t , :],one_full_traj[:, t : t + self.ws, 0:1]), dim=2)\n",
    "                \n",
    "                lstm_out, hidden = self.lstm(seq)           \n",
    "                pred = self.linear(lstm_out)\n",
    "\n",
    "                out = torch.cat((out, out[:, t-1:t, :] + self.step_size * pred[:, -1: , :]), dim=1)\n",
    "\n",
    "        if RK == self.rungekutta:\n",
    "            for t in range(1, self.ws + 2): # f체r RK : range(1, self.ws + 2):\n",
    "\n",
    "                tmp = torch.cat(( one_full_traj[:,self.ws+t:self.ws+t+1, 0:1] , out[:, (t-1):t,:]), dim=2)\n",
    "                seq = torch.cat((one_full_traj[:, t:self.ws+(t-1), :], tmp), dim=1)\n",
    "                lstm_out, hidden = self.lstm(seq)           \n",
    "                pred = self.linear(lstm_out)\n",
    "                out = torch.cat((out, one_full_traj[:, self.ws+(t-1): self.ws+t, 1:] + pred[:, -1: , :]), dim=1)\n",
    "            \n",
<<<<<<< HEAD
    "            for t in range(self.ws, one_full_traj.size(dim=1) - self.ws - 2):\n",
    "                # seq = torch.cat((out[:, t - self.ws : t , :], one_full_traj[:, t : t + self.ws, 0:1]), dim=2)\n",
    "                \n",
    "                # lstm_out, hidden = self.lstm(seq)           \n",
    "                # pred = self.linear(lstm_out)\n",
    "\n",
    "                # out = torch.cat((out, out[:, t-1:t, :] + pred[:, -1: , :]), dim=1)\n",
    "\n",
    "                #Runge Kutta : \n",
    "                \n",
    "                #y(n+1) = y(n) + h/6 * (k1 + 2k2 + 2k3 + k4)\n",
    "                # k1 = f(y(n))          --- u1\n",
    "                # k2 = f(y(n)+h/2*k1)   --- u2\n",
    "                # k3 = f(y(n)+h/2*k2)   --- u2\n",
    "                # k4 = f(y(n)+h*k3)     --- u3\n",
    "                # We only have u at discrete steps -> use h = 2 such that y(n)+k1 = y(n+1) and so on\n",
    "\n",
    "                #richtig w채re :\n",
    "                seq = torch.cat((one_full_traj[:, t : t + self.ws + 2, 0:1], out[:, t - self.ws : t + 2 , :]), dim=2)\n",
    "                #war falsch beim training!\n",
    "                #seq = torch.cat((out[:, t - self.ws : t + 2 , :], one_full_traj[:, t : t + self.ws + 2, 0:1]), dim=2)\n",
    "\n",
    "                inp1 = seq[:, 0:-2, :]\n",
    "\n",
    "                lstm_out, hidden = self.lstm(inp1)           \n",
    "                k1 = self.linear(lstm_out)\n",
    "\n",
    "                inp2 = seq[:, 1:-1, :]\n",
    "                inp2[:, -1:, 1:] = inp2[:, -1:, 1:] + k1[:, -1, :]\n",
    "\n",
    "                lstm_out, hidden = self.lstm(inp2)           \n",
    "                k2 = self.linear(lstm_out) \n",
    "\n",
    "                inp3 = seq[:, 1:-1, :]\n",
    "                inp3[:, -1:, 1:] = inp3[:, -1:, 1:] + k2[:, -1, :]\n",
    "\n",
    "                lstm_out, hidden = self.lstm(inp3)           \n",
    "                k3 = self.linear(lstm_out)          \n",
    "\n",
    "                inp4 = seq[:, 2:, :]\n",
    "                inp4[:, -1:, 1:] = inp4[:, -1:, 1:] + 2*k3[:, -1, :]\n",
    "\n",
    "                lstm_out, hidden = self.lstm(inp4)           \n",
    "                k4 = self.linear(lstm_out)  \n",
    "\n",
    "                    # y(n+1) ist 2 steps in der zukunft wegen h = 2 ?!?\n",
    "                res = out[:, t:t+1, :]  +  2/6 * (k1[:, -1, :] + 2*k2[:, -1, :] + 2*k3[:, -1, :] + k4[:, -1, :])\n",
    "                out = torch.cat((out, res), dim=1)\n",
=======
    "            lstm_out, hidden = self.lstm(seq)           \n",
    "            pred = self.linear(lstm_out)\n",
    "            #only update next step\n",
    "            out = torch.cat((out, out[:, t-1:t, :] + pred[:, -1: , :]), dim=1)\n",
    "            #out = torch.cat((out, out[:, t-self.ws:t, :] + pred), dim=1)\n",
>>>>>>> 56617d4d212940381c717bbfd9bee16f3ba39408
    "\n",
    "            \n",
    "        return out, hidden          \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 37,
>>>>>>> 56617d4d212940381c717bbfd9bee16f3ba39408
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[[1,2],[1,2],[1,2],[1,2],[1,2]]])\n",
    "b = torch.tensor([[[1,2,9],[1,2,9],[1,2,9],[1,2,9],[1,2,9]]])\n",
    "c = torch.cat((a[:, 0 : 4, :], b[:, 0 : 4, 2:3]), dim=2)\n",
    "a = a[:,0:-2,:]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
>>>>>>> 291077b3698dc9667d8383b80f68e8b1ec597541
   "metadata": {},
   "outputs": [],
   "source": [
    "#with future:\n",
    " \n",
    "def train(input_data, model, weight_decay, learning_rate=0.001, ws=0):\n",
    " \n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay=weight_decay)\n",
    " \n",
    "    model.train()\n",
    "    total_loss = []\n",
    "  \n",
    "    for k, (x,y) in enumerate(input_data):  # inp = (u, x) label = x\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        output, _ = model(x)\n",
    "  \n",
    "        # reset the gradient\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # calculate the error\n",
    "        loss = loss_fn(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    " \n",
    "        total_loss.append(loss.detach().cpu().numpy())\n",
    " \n",
    "   # return the average error of the next step prediction\n",
    "    return np.mean(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 550, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]c:\\Users\\StrasserP\\AppData\\Local\\anaconda3\\envs\\cuda_env\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1, 346, 2])) that is different to the input size (torch.Size([1, 1384, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1384) must match the size of tensor b (346) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 63\u001b[0m\n\u001b[0;32m     59\u001b[0m average_traj_err_test \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m])):\n\u001b[1;32m---> 63\u001b[0m     loss_epoch \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwindow_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss_epoch)\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Every few epochs get the error MSE of the true data\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# compared to the network prediction starting from some initial conditions\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 22\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(input_data, model, weight_decay, learning_rate, ws)\u001b[0m\n\u001b[0;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# calculate the error\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\StrasserP\\AppData\\Local\\anaconda3\\envs\\cuda_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\StrasserP\\AppData\\Local\\anaconda3\\envs\\cuda_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\StrasserP\\AppData\\Local\\anaconda3\\envs\\cuda_env\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\StrasserP\\AppData\\Local\\anaconda3\\envs\\cuda_env\\Lib\\site-packages\\torch\\nn\\functional.py:3338\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3336\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3338\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[1;32mc:\\Users\\StrasserP\\AppData\\Local\\anaconda3\\envs\\cuda_env\\Lib\\site-packages\\torch\\functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (1384) must match the size of tensor b (346) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "\n",
    "params =           {\n",
    "                           \"experiment_number\" : 2,\n",
    "                           \"window_size\" : 4,\n",
    "                           \"h_size\" : 5,\n",
    "                           \"l_num\" : 1,\n",
    "                           \"epochs\" : 10,\n",
    "                           \"learning_rate\" : 0.0007,\n",
    "                           \"part_of_data\" : 10, \n",
    "                           \"weight_decay\" : 1e-5,\n",
    "                           \"percentage_of_data\" : 0.6,\n",
    "                           \"future_decay\"  : 0.5,\n",
    "                           \"batch_size\" : 1,\n",
    "                           \"future\" : 10,\n",
    "                           \"cut_off_timesteps\" : 200,\n",
    "                           \"drop_half_timesteps\": True\n",
    "                        }\n",
    "\n",
    "\n",
    "# Initialize the LSTM model\n",
    "model = LSTMmodel(input_size=3, hidden_size=params[\"h_size\"], out_size=2, layers=params[\"l_num\"], window_size=params[\"window_size\"], rungekutta=False).to(device)\n",
    "\n",
    "# Generate input data (the data is normalized and some timesteps are cut off)\n",
    "input_data, PSW_max = get_data(path = \"save_data_test5.csv\", \n",
    "                        timesteps_from_data=0, \n",
    "                        skip_steps_start = 0,\n",
    "                        skip_steps_end = 0, \n",
    "                        drop_half_timesteps = params[\"drop_half_timesteps\"],\n",
    "                        normalise_s_w=\"minmax\",\n",
    "                        rescale_p=False,\n",
    "                        num_inits=params[\"part_of_data\"])\n",
    "\n",
    "input_data2, PSW_max = get_data(path = \"Testruns_from_trajectory_generator_200.csv\", \n",
    "                        timesteps_from_data=0, \n",
    "                        skip_steps_start = 0,\n",
    "                        skip_steps_end = 0, \n",
    "                        drop_half_timesteps = params[\"drop_half_timesteps\"],\n",
    "                        normalise_s_w=\"minmax\",\n",
    "                        rescale_p=False,\n",
    "                        num_inits=params[\"part_of_data\"])\n",
    "\n",
    "input_data = torch.cat((input_data, input_data2))\n",
    "print(input_data.size())\n",
    "\n",
    "#Split data into train and test sets\n",
    "np.random.seed(1234)\n",
    "num_of_inits_train = int(len(input_data)*params[\"percentage_of_data\"])\n",
    "train_inits = np.random.choice(np.arange(len(input_data)),num_of_inits_train,replace=False)\n",
    "test_inits = np.array([x for x in range(len(input_data)) if x not in train_inits])\n",
    "\n",
    "train_data = input_data[train_inits,:input_data.size(dim=1)-params[\"cut_off_timesteps\"],:]\n",
    "test_data = input_data[test_inits,:,:]\n",
    "\n",
    "# dataloader for batching during training\n",
    "train_set = custom_simple_dataset(train_data, window_size=params[\"window_size\"])\n",
    "train_loader = DataLoader(train_set, batch_size=params[\"batch_size\"], pin_memory=True)\n",
    "\n",
    "losses = []\n",
    "average_traj_err_train = []\n",
    "average_traj_err_test = []\n",
    "\n",
    "for e in tqdm(range(params[\"epochs\"])):\n",
    "    \n",
    "    loss_epoch = train(train_loader, model, params[\"weight_decay\"], learning_rate= params[\"learning_rate\"], ws=params[\"window_size\"])\n",
    "    losses.append(loss_epoch)\n",
    "\n",
    "    # Every few epochs get the error MSE of the true data\n",
    "    # compared to the network prediction starting from some initial conditions\n",
    "    if (e+1)%20 == 0:\n",
    "\n",
    "        _,_, err_train = test(input_data, model, model_type = \"or_lstm\", window_size=params[\"window_size\"], display_plots=False, num_of_inits = 20, set_rand_seed=True, physics_rescaling = PSW_max)\n",
    "\n",
    "        average_traj_err_train.append(err_train)\n",
    "        #average_traj_err_test.append(err_test)\n",
    "        print(f\"Epoch: {e}, the average next step error was : loss_epoch\")\n",
    "        print(f\"Average error over full trajectories: training data : {err_train}\")\n",
    "                #print(f\"Average error over full trajectories: testing data : {err_test}\")\n",
    "\n",
    "_,_, err_test = test(input_data, model, model_type = \"or_lstm\", window_size=params[\"window_size\"], display_plots=False, num_of_inits = 100, set_rand_seed=True, physics_rescaling = PSW_max)\n",
    "#_,_, err_test = test(test_data, model, steps=test_data.size(dim=1), ws=window_size, plot_opt=False, n = 100)\n",
    "print(f\"TRAINING FINISHED: Average error over full trajectories: testing data : {err_test}\")\n",
    "#print(f\"TRAINING FINISHED: Average error over full trajectories: testing data : {err_test}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.670050308166006\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "#path = f\"Ventil_trained_NNs\\lstm_ws0.pth\"\n",
    "#torch.save(model.state_dict(), path)\n",
    "\n",
    "# Load the model and test it on the test data\n",
    "path = \"working_networks\\OR_lstm_trained.pth\"\n",
    "\n",
    "\n",
    "params =                 {\n",
    "                           \"experiment_number\" : 2,\n",
    "                           \"window_size\" : 4,\n",
    "                           \"h_size\" : 5,\n",
    "                           \"l_num\" : 1,\n",
    "                           \"epochs\" : 4000,\n",
    "                           \"learning_rate\" : 0.0008,\n",
    "                           \"part_of_data\" : 0, \n",
    "                           \"weight_decay\" : 1e-5,\n",
    "                           \"percentage_of_data\" : 0.8,\n",
    "                           \"future_decay\"  : 0.5,\n",
    "                           \"batch_size\" : 20,\n",
    "                           \"future\" : 10,\n",
    "                           \"cut_off_timesteps\" : 0,\n",
    "                           \"drop_half_timesteps\": True\n",
    "                        }\n",
    "\n",
    "\n",
    "input_data, PSW_max = get_data(path = \"save_data_test4.csv\", \n",
    "                        timesteps_from_data=0, \n",
    "                        skip_steps_start = 0,\n",
    "                        skip_steps_end = 0, \n",
    "                        drop_half_timesteps = params[\"drop_half_timesteps\"],\n",
    "                        normalise_s_w=\"minmax\",\n",
    "                        rescale_p=False,\n",
    "                        num_inits=params[\"part_of_data\"])\n",
    "\n",
    "input_data2, PSW_max = get_data(path =  r\"Testruns_from_trajectory_generator.csv\", \n",
    "                        timesteps_from_data=0, \n",
    "                        skip_steps_start = 0,\n",
    "                        skip_steps_end = 0, \n",
    "                        drop_half_timesteps = params[\"drop_half_timesteps\"],\n",
    "                        normalise_s_w=\"minmax\",\n",
    "                        rescale_p=False,\n",
    "                        num_inits=params[\"part_of_data\"])\n",
    "\n",
    "input_data = torch.cat((input_data[:,:,:], input_data2[-1:,:,:]))\n",
    "\n",
    "#input_data = torch.cat((input_data, input_data2, input_data3))\n",
    "\n",
    "#np.random.seed(1234)\n",
    "\n",
    "num_of_inits_train = int(len(input_data)*params[\"percentage_of_data\"])\n",
    "train_inits = np.random.choice(np.arange(len(input_data)),num_of_inits_train,replace=False)\n",
    "test_inits = np.array([x for x in range(len(input_data)) if x not in train_inits])\n",
    "test_data = input_data[test_inits,:,:]\n",
    "# Initialize the LSTM model\n",
    "\n",
    "model = LSTMmodel(input_size=3, hidden_size=params[\"h_size\"], out_size=2, layers=params[\"l_num\"], window_size=params[\"window_size\"], stepsize=1).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(path, map_location=torch.device(device)))\n",
    "\n",
    "train_data = input_data[train_inits,:,:]\n",
    "%matplotlib qt \n",
    "test_loss, test_loss_deriv, total_loss = test(input_data, model, model_type = \"or_lstm\", window_size=params[\"window_size\"], display_plots=True, num_of_inits = 1, set_rand_seed=True, physics_rescaling = PSW_max)\n",
    "#test_loss, test_loss_deriv, total_loss = test(train_data, model, steps=input_data.size(dim=1), ws=params[\"window_size\"], plot_opt=True , n = 1,  test_inits=len(test_data), rand=False, PSW_max=0)\n",
    "print(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # def forward(self, one_full_traj):\n",
    "\n",
    "    #     seq = one_full_traj[:, 0:self.ws, :]\n",
    "    #     lstm_out, hidden = self.lstm(seq)           \n",
    "    #     pred = self.linear(lstm_out)\n",
    "    #     #only update next step\n",
    "    #     out = one_full_traj[:, self.ws-1:self.ws, 1:] + pred[:, -1: , :]\n",
    "    #     #out = one_full_traj[:, self.ws-1:self.ws, 1:] + pred\n",
    "\n",
    "    #     for t in range(1, self.ws):\n",
    "\n",
    "    #         tmp = torch.cat(( one_full_traj[:,self.ws+t:self.ws+t+1, 0:1] , out[:, (t-1):t,:]), dim=2)\n",
    "    #         seq = torch.cat((one_full_traj[:, t:self.ws+(t-1), :], tmp), dim=1)\n",
    "    #         lstm_out, hidden = self.lstm(seq)           \n",
    "    #         pred = self.linear(lstm_out)\n",
    "    #         #only update next step\n",
    "    #         out = torch.cat((out, one_full_traj[:, self.ws+(t-1): self.ws+t, 1:] +  pred[:, -1: , :]), dim=1)\n",
    "    #        # out = torch.cat((out, one_full_traj[:, self.ws+(t-1): self.ws+t, 1:] +  pred), dim=1)\n",
    "\n",
    "    #     for t in range(self.ws, one_full_traj.size(dim=1) - self.ws):\n",
    "    #         seq = torch.cat((out[:, t - self.ws : t , :], one_full_traj[:, t : t + self.ws, 0:1]), dim=2)\n",
    "            \n",
    "    #         lstm_out, hidden = self.lstm(seq)           \n",
    "    #         pred = self.linear(lstm_out)\n",
    "    #         #only update next step\n",
    "    #         out = torch.cat((out, out[:, t-1:t, :] + pred[:, -1: , :]), dim=1)\n",
    "    #         #out = torch.cat((out, out[:, t-self.ws:t, :] + pred), dim=1)\n",
    "\n",
    "\n",
    "    #     return out, hidden   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
