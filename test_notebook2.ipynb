{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from matplotlib import legend\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import numpy as np\n",
    "from icecream import ic\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from get_data import get_data\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    " #Define the LSTM model with two hidden layers\n",
    "torch.set_default_dtype(torch.float64)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "class LSTMmodel(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM model class for derivative estimation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, out_size, layers):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM model.\n",
    "\n",
    "        Args:\n",
    "        - input_size: Size of input\n",
    "        - hidden_size: Size of hidden layer\n",
    "        - out_size: Size of output\n",
    "        - layers: Number of layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # Define LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=layers, batch_first=True)\n",
    "\n",
    "        # Define linear layer\n",
    "        self.linear = nn.Linear(hidden_size, out_size)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        \"\"\"\n",
    "        Forward pass through the LSTM model.\n",
    "\n",
    "        Args:\n",
    "        - seq: Input sequence\n",
    "\n",
    "        Returns:\n",
    "        - pred: Model prediction\n",
    "        - hidden: Hidden state\n",
    "        \"\"\"\n",
    "        lstm_out, hidden = self.lstm(seq)\n",
    "        pred = self.linear(lstm_out)\n",
    "\n",
    "        return pred, hidden\n",
    "\n",
    "\n",
    "class GRUmodel(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM model class for derivative estimation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, out_size, layers):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM model.\n",
    "\n",
    "        Args:\n",
    "        - input_size: Size of input\n",
    "        - hidden_size: Size of hidden layer\n",
    "        - out_size: Size of output\n",
    "        - layers: Number of layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # Define LSTM layer\n",
    "        self.lstm = nn.GRU(input_size, hidden_size, num_layers=layers, batch_first=True)\n",
    "\n",
    "        # Define linear layer\n",
    "        self.linear = nn.Linear(hidden_size, out_size)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        \"\"\"\n",
    "        Forward pass through the LSTM model.\n",
    "\n",
    "        Args:\n",
    "        - seq: Input sequence\n",
    "\n",
    "        Returns:\n",
    "        - pred: Model prediction\n",
    "        - hidden: Hidden state\n",
    "        \"\"\"\n",
    "        lstm_out, hidden = self.lstm(seq)\n",
    "        pred = self.linear(lstm_out)\n",
    "\n",
    "        return pred, hidden\n",
    "\n",
    "\n",
    "def test(test_data, model, steps=600, ws=10, plot_opt=False):\n",
    "\n",
    "    #test_data = test_dataloader.get_all_data() \n",
    "    model.eval()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    test_loss = 0\n",
    "    test_loss_deriv = 0\n",
    "    loss_list=[]\n",
    "    randoms = np.random.randint(0,100,10)\n",
    "    plotted=0\n",
    "    for i, x in enumerate(test_data):\n",
    "        x=x.to(device)\n",
    "        if i not in randoms:\n",
    "            continue\n",
    "\n",
    "        with torch.inference_mode():\n",
    "\n",
    "            pred = torch.zeros((steps, 3), device=device)\n",
    "            pred_next_step = torch.zeros((steps, 3), device=device)\n",
    "\n",
    "            if ws > 1:\n",
    "                pred[0:ws, :] = x[0:ws, :]\n",
    "                pred[:, 0] = x[:, 0]\n",
    "                pred_next_step[0:ws, :] = x[0:ws, :]\n",
    "                pred_next_step[:, 0] = x[:, 0]\n",
    "            else:\n",
    "                pred[0, :] = x[0, :]\n",
    "                pred[:, 0] = x[:, 0]\n",
    "                pred_next_step[0, :] = x[0, :]\n",
    "                pred_next_step[:, 0] = x[:, 0]\n",
    "\n",
    "            for i in range(len(x) - ws):\n",
    "\n",
    "                out, _ = model(pred[i:i+ws, :])\n",
    "                pred[i+ws, 1:] = pred[i+ws-1, 1:] + out[-1, :]\n",
    "                pred_next_step[i+ws, 1:] = x[i+ws-1, 1:] + out[-1, :]\n",
    "            \n",
    "            loss_list.append((loss_fn(pred[:, 1], x[:, 1]).detach().cpu().numpy(),loss_fn(pred[:, 2], x[:, 2]).detach().cpu().numpy()))\n",
    "            test_loss += loss_fn(pred[:, 1], x[:, 1]).detach().cpu().numpy()\n",
    "            test_loss_deriv += loss_fn(pred[:, 2], x[:, 2]).detach().cpu().numpy()\n",
    "\n",
    "            if plot_opt and plotted==0:\n",
    "                plotted=1\n",
    "                figure , axs = plt.subplots(1,3,figsize=(16,9))\n",
    "            \n",
    "                axs[0].plot(pred.detach().cpu().numpy()[:, 1], color=\"red\", label=\"pred\")\n",
    "                axs[0].plot(pred_next_step.detach().cpu().numpy()[:, 1], color=\"green\", label=\"next step from data\")\n",
    "                axs[0].plot(x.detach().cpu().numpy()[:, 1], color=\"blue\", label=\"true\", linestyle=\"dashed\")\n",
    "                axs[0].set_title(\"position\")\n",
    "                axs[0].grid()\n",
    "                axs[0].legend()\n",
    "\n",
    "                axs[1].plot(pred.detach().cpu().numpy()[:, 2], color=\"red\", label=\"pred\")\n",
    "                axs[1].plot(pred_next_step.detach().cpu().numpy()[:, 2], color=\"green\", label=\"next step from data\")\n",
    "                axs[1].plot(x.detach().cpu().numpy()[:, 2], color=\"blue\", label=\"true\", linestyle=\"dashed\")\n",
    "                axs[1].set_title(\"speed\")\n",
    "                axs[1].grid()\n",
    "                axs[1].legend()\n",
    "\n",
    "                axs[2].plot(x.detach().cpu().numpy()[:,0], label=\"pressure\")\n",
    "                axs[2].set_title(\"pressure\")\n",
    "                axs[2].grid()\n",
    "                axs[2].legend()\n",
    "\n",
    "                plt.grid()\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "            \n",
    "    return np.mean(test_loss), np.mean(test_loss_deriv), loss_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path = \"ventil_lstm\\save_data_test.csv\", timesteps_from_data=100, skip_steps_start = 1, skip_steps_end = 1, drop_half_timesteps = True, normalise_s_w=False, rescale_p=False, num_inits=0):\n",
    "    \n",
    "    if timesteps_from_data>1:\n",
    "     df = pd.read_csv(path, header=0, nrows=timesteps_from_data, skiprows=skip_steps_start)\n",
    "    else:\n",
    "     df = pd.read_csv(path, header=0, skiprows=skip_steps_start)\n",
    "\n",
    "    if skip_steps_end>1:\n",
    "       df = df.iloc[0:len(df)-skip_steps_end]\n",
    "\n",
    "    #drop even more timesteps\n",
    "    if drop_half_timesteps:\n",
    "     df = df.iloc[::2]\n",
    "\n",
    "    if num_inits>1:\n",
    "       df = df.iloc[:,0:4*num_inits]\n",
    "    #Reorder columns for familiar setup (t,u,x) here (t, p_b, s_b, w_b)\n",
    "    L = df.columns.to_list()\n",
    "    time_cols = L[0::4]\n",
    "    sb_cols = L[1::4]\n",
    "    pb_cols = L[2::4]\n",
    "    wb_cols = L[3::4]\n",
    "    new_col_order = [x for sub in list(zip(time_cols, pb_cols, sb_cols, wb_cols)) for x in sub]\n",
    "    df= df[new_col_order]\n",
    "    df = df.drop(time_cols, axis=1)\n",
    "\n",
    "    #normalise each column of the dataframe\n",
    "    #mean normalization\n",
    "    #df=(df-df.mean())/df.std()\n",
    "    \n",
    "    #Can't normalize p_b because then a[i]*X+b[i] becomes cX+d for all i.. same with mean normal. \n",
    "    \n",
    "    # Normalise / Rescale\n",
    "    if normalise_s_w:\n",
    "        tmp=pb_cols+sb_cols+wb_cols\n",
    "        df[tmp]=(df[tmp]-df[tmp].min())/(df[tmp].max()-df[tmp].min())\n",
    "    if rescale_p:\n",
    "        df[pb_cols] = df[pb_cols] / 1e5\n",
    "\n",
    "    tensor = torch.tensor(df.values)\n",
    "\n",
    "    #tensor with t=0:600, 500 different input and the 3 outputs [s_b, p_b, w_b]\n",
    "    a = num_inits if num_inits>0 else 500\n",
    "    a=int(len(df.columns.to_list())/3)\n",
    "\n",
    "    tensor = tensor.view(len(df),a,3).permute(1,0,2)\n",
    "\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Ventil_trained_NNs\\\\lstm_ws4hs64layer3_nummer329.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 29\u001b[0m\n\u001b[0;32m     25\u001b[0m model \u001b[38;5;241m=\u001b[39m LSTMmodel(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39mh_size, out_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, layers\u001b[38;5;241m=\u001b[39ml_num)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     27\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVentil_trained_NNs\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mlstm_ws\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwindow_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mhs\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_nummer329.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 29\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     31\u001b[0m a, b, loss_list\u001b[38;5;241m=\u001b[39m test(input_data, model, steps\u001b[38;5;241m=\u001b[39minput_data\u001b[38;5;241m.\u001b[39msize(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), ws\u001b[38;5;241m=\u001b[39mwindow_size, plot_opt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss_list)\n",
      "File \u001b[1;32mc:\\Users\\strasserp\\AppData\\Local\\anaconda3\\envs\\my_test_env\\Lib\\site-packages\\torch\\serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    984\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 986\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    988\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    989\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    990\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    991\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\strasserp\\AppData\\Local\\anaconda3\\envs\\my_test_env\\Lib\\site-packages\\torch\\serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 435\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\strasserp\\AppData\\Local\\anaconda3\\envs\\my_test_env\\Lib\\site-packages\\torch\\serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 416\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Ventil_trained_NNs\\\\lstm_ws4hs64layer3_nummer329.pth'"
     ]
    }
   ],
   "source": [
    "input_data = get_data(path = \"save_data_test3.csv\", \n",
    "                                timesteps_from_data=0, \n",
    "                                skip_steps_start = 100,\n",
    "                                skip_steps_end = 0, \n",
    "                                drop_half_timesteps = True,\n",
    "                                normalise_s_w=True,\n",
    "                                rescale_p=False,\n",
    "                                num_inits=0)\n",
    "\n",
    "input_data=input_data.to(device)\n",
    "# Split data into train and test sets\n",
    "train_size = int(0.7 * len(input_data))\n",
    "test_size = len(input_data) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(input_data, [train_size, test_size])\n",
    "\n",
    "# Take a slice of data for training (only slice_of_data many timesteps)\n",
    "slice_of_data = 300\n",
    "\n",
    "#train_dataset = train_dataset[:][:, 0:slice_of_data, :]\n",
    "\n",
    "\n",
    "window_size = 4\n",
    "h_size=64\n",
    "l_num=3\n",
    "model = LSTMmodel(input_size=3, hidden_size=h_size, out_size=2, layers=l_num).to(device)\n",
    "\n",
    "path = f\"Ventil_trained_NNs\\lstm_ws{window_size}hs{h_size}layer{l_num}_nummer329.pth\"\n",
    "\n",
    "model.load_state_dict(torch.load(path, map_location=torch.device(device)))\n",
    "\n",
    "a, b, loss_list= test(input_data, model, steps=input_data.size(dim=1), ws=window_size, plot_opt=True)\n",
    "print(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import legend\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import numpy as np\n",
    "from icecream import ic\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from get_data import get_data\n",
    "import logging\n",
    "import os\n",
    "import cProfile\n",
    "import pstats\n",
    "from dataloader import *\n",
    "\n",
    "\n",
    "losses = []\n",
    "\n",
    "# Generate input data\n",
    "input_data = get_data(path = \"save_data_test3.csv\", \n",
    "                        timesteps_from_data=0, \n",
    "                        skip_steps_start = 0,\n",
    "                        skip_steps_end = 0, \n",
    "                        drop_half_timesteps = True,\n",
    "                        normalise_s_w=True,\n",
    "                        rescale_p=False,\n",
    "                        num_inits=40)\n",
    "\n",
    "\n",
    "\n",
    "data  = CustomDataset(input_data, window_size=4)\n",
    "\n",
    "#Split data into train and test sets\n",
    "train_size = int(0.7 * len(data) / 64) * 64\n",
    "test_size = len(data) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(data, [train_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64,pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_data:\n",
    "    plt.plot(x[:,0].numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws=4\n",
    "\n",
    "train_inits = np.random.randint(0,len(input_data), 10)\n",
    "train_inits = np.unique(train_inits)\n",
    "test_inits = np.array([x for x in range(len(input_data)) if x not in train_inits])\n",
    "\n",
    "train_data = input_data[train_inits,:,:]\n",
    "test_data = input_data[test_inits,:,:]\n",
    "\n",
    "data_set  = CustomDataset(train_data, window_size=4)\n",
    "\n",
    "train_dataloader = DataLoader(data_set, batch_size=64,pin_memory=True)\n",
    "\n",
    "time = np.linspace(0,input_data.size(dim=1)*len(input_data),input_data.size(dim=1)*len(input_data))\n",
    "\n",
    "\n",
    "for i, (inp, label) in enumerate(train_dataloader):\n",
    "    if i%2==0:\n",
    "     plt.plot(time[i:i+ws], inp[0,:,0].numpy(), linewidth=5, alpha=0.2, color=\"red\")\n",
    "     #plt.scatter(time[i:i+ws], label[0,:,0].numpy(), linewidth=5, alpha=0.3, color=\"red\")\n",
    "     \n",
    "    else:\n",
    "     plt.plot(time[i:i+ws], inp[0,:,0].numpy(), linewidth=5, alpha=0.2, color=\"blue\")\n",
    "\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_of_inits_train = 500\n",
    "percentage = 0.9\n",
    "\n",
    "train_inits = np.random.randint(0,num_of_inits_train, num_of_inits_train)\n",
    "train_inits = np.unique(train_inits)\n",
    "test_inits = np.array([x for x in range(num_of_inits_train) if x not in train_inits])\n",
    "\n",
    "print(len(train_inits), len(test_inits))\n",
    "\n",
    "ic(train_inits, test_inits)\n",
    "if percentage < 0.99:\n",
    "    while len(train_inits) < num_of_inits_train * percentage:\n",
    "        i = np.random.randint(0,len(test_inits),1)[0]\n",
    "        train_inits = np.append(train_inits,test_inits[i])\n",
    "        test_inits = np.delete(test_inits, i)\n",
    "\n",
    "ic( train_inits)\n",
    "\n",
    "ic(test_inits)    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
