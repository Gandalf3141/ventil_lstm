{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mamba tiny\n",
    "\"\"\"Simple, minimal implementation of Mamba in one file of PyTorch.\n",
    "\n",
    "Suggest reading the following before/while reading the code:\n",
    "    [1] Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Albert Gu and Tri Dao)\n",
    "        https://arxiv.org/abs/2312.00752\n",
    "    [2] The Annotated S4 (Sasha Rush and Sidd Karamcheti)\n",
    "        https://srush.github.io/annotated-s4\n",
    "\n",
    "Glossary:\n",
    "    b: batch size                       (`B` in Mamba paper [1] Algorithm 2)\n",
    "    l: sequence length                  (`L` in [1] Algorithm 2)\n",
    "    d or d_model: hidden dim\n",
    "    n or d_state: latent state dim      (`N` in [1] Algorithm 2)\n",
    "    expand: expansion factor            (`E` in [1] Section 3.4)\n",
    "    d_in or d_inner: d * expand         (`D` in [1] Algorithm 2)\n",
    "    A, B, C, D: state space parameters  (See any state space representation formula)\n",
    "                                        (B, C are input-dependent (aka selective, a key innovation in Mamba); A, D are not)\n",
    "    Δ or delta: input-dependent step size\n",
    "    dt_rank: rank of Δ                  (See [1] Section 3.6 \"Parameterization of ∆\")\n",
    "\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from scans import selective_scan\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    d_model: int\n",
    "    n_layer: int\n",
    "    vocab_size: int\n",
    "    d_state: int = 16\n",
    "    expand: int = 2\n",
    "    dt_rank: Union[int, str] = 'auto'\n",
    "    d_conv: int = 4 \n",
    "    pad_vocab_size_multiple: int = 8\n",
    "    conv_bias: bool = True\n",
    "    bias: bool = False\n",
    "    scan_mode: str = 'cumsum'\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.d_inner = int(self.expand * self.d_model)\n",
    "        \n",
    "        if self.dt_rank == 'auto':\n",
    "            self.dt_rank = math.ceil(self.d_model / 16)\n",
    "            \n",
    "        if self.vocab_size % self.pad_vocab_size_multiple != 0:\n",
    "            self.vocab_size += (self.pad_vocab_size_multiple\n",
    "                                - self.vocab_size % self.pad_vocab_size_multiple)\n",
    "\n",
    "\n",
    "class Mamba(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        \"\"\"Full Mamba model.\"\"\"\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        \n",
    "        self.embedding = nn.Embedding(args.vocab_size, args.d_model)\n",
    "        self.layers = nn.ModuleList([ResidualBlock(args) for _ in range(args.n_layer)])\n",
    "        self.norm_f = RMSNorm(args.d_model)\n",
    "\n",
    "        self.lm_head = nn.Linear(args.d_model, args.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.embedding.weight  # Tie output projection to embedding weights.\n",
    "                                                     # See \"Weight Tying\" paper\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids (long tensor): shape (b, l)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "    \n",
    "        Returns:\n",
    "            logits: shape (b, l, vocab_size)\n",
    "\n",
    "        Official Implementation:\n",
    "            class MambaLMHeadModel, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/models/mixer_seq_simple.py#L173\n",
    "\n",
    "        \"\"\"\n",
    "        x = self.embedding(input_ids)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        x = self.norm_f(x)\n",
    "        return self.lm_head(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_pretrained(pretrained_model_name: str, model=None):\n",
    "        \"\"\"Load pretrained weights from HuggingFace into model.\n",
    "    \n",
    "        Args:\n",
    "            pretrained_model_name: One of\n",
    "                * 'state-spaces/mamba-2.8b-slimpj'\n",
    "                * 'state-spaces/mamba-2.8b'\n",
    "                * 'state-spaces/mamba-1.4b'\n",
    "                * 'state-spaces/mamba-790m'\n",
    "                * 'state-spaces/mamba-370m'\n",
    "                * 'state-spaces/mamba-130m'\n",
    "                            \n",
    "        Returns:\n",
    "            model: Mamba model with weights loaded\n",
    "    \n",
    "        \"\"\"\n",
    "        from transformers.utils import CONFIG_NAME, WEIGHTS_NAME\n",
    "        from transformers.utils.hub import cached_file\n",
    "        \n",
    "        def load_config_hf(model_name):\n",
    "            resolved_archive_file = cached_file(model_name, CONFIG_NAME,\n",
    "                                                _raise_exceptions_for_missing_entries=False)\n",
    "            return json.load(open(resolved_archive_file))\n",
    "        \n",
    "        \n",
    "        def load_state_dict_hf(model_name, device=None, dtype=None):\n",
    "            resolved_archive_file = cached_file(model_name, WEIGHTS_NAME,\n",
    "                                                _raise_exceptions_for_missing_entries=False)\n",
    "            return torch.load(resolved_archive_file, weights_only=True, map_location='cpu', mmap=True)\n",
    "        \n",
    "        if model is None:\n",
    "            config_data = load_config_hf(pretrained_model_name)\n",
    "            model = Mamba(ModelArgs(\n",
    "                d_model=config_data['d_model'], \n",
    "                n_layer=config_data['n_layer'], \n",
    "                vocab_size=config_data['vocab_size'], \n",
    "            ))\n",
    "        \n",
    "        pretrained_dict = load_state_dict_hf(pretrained_model_name)\n",
    "        model_dict = model.state_dict()\n",
    "        \n",
    "        for k, v in pretrained_dict.items():\n",
    "            k_new = k.replace('backbone.', '')\n",
    "            if k_new in model_dict and v.size() == model_dict[k_new].size():\n",
    "                model_dict[k_new] = pretrained_dict[k]\n",
    "        \n",
    "        model.load_state_dict(model_dict)\n",
    "        return model\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        \"\"\"Simple block wrapping Mamba block with normalization and residual connection.\"\"\"\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.mixer = MambaBlock(args)\n",
    "        self.norm = RMSNorm(args.d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: shape (b, l, d)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "    \n",
    "        Returns:\n",
    "            output: shape (b, l, d)\n",
    "\n",
    "        Official Implementation:\n",
    "            Block.forward(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L297\n",
    "            \n",
    "            Note: the official repo chains residual blocks that look like\n",
    "                [Add -> Norm -> Mamba] -> [Add -> Norm -> Mamba] -> [Add -> Norm -> Mamba] -> ...\n",
    "            where the first Add is a no-op. This is purely for performance reasons as this\n",
    "            allows them to fuse the Add->Norm.\n",
    "\n",
    "            We instead implement our blocks as the more familiar, simpler, and numerically equivalent\n",
    "                [Norm -> Mamba -> Add] -> [Norm -> Mamba -> Add] -> [Norm -> Mamba -> Add] -> ....\n",
    "            \n",
    "        \"\"\"\n",
    "        return self.mixer(self.norm(x)) + x\n",
    "            \n",
    "\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        \"\"\"A single Mamba block, as described in Figure 3 in Section 3.4 in the Mamba paper [1].\"\"\"\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.in_proj = nn.Linear(args.d_model, args.d_inner * 2, bias=args.bias)\n",
    "\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=args.d_inner,\n",
    "            out_channels=args.d_inner,\n",
    "            bias=args.conv_bias,\n",
    "            kernel_size=args.d_conv,\n",
    "            groups=args.d_inner,\n",
    "            padding=args.d_conv - 1,\n",
    "        )\n",
    "\n",
    "        # x_proj takes in `x` and outputs the input-specific Δ, B, C\n",
    "        self.x_proj = nn.Linear(args.d_inner, args.dt_rank + args.d_state * 2, bias=False)\n",
    "        \n",
    "        # dt_proj projects Δ from dt_rank to d_in\n",
    "        self.dt_proj = nn.Linear(args.dt_rank, args.d_inner, bias=True)\n",
    "\n",
    "        A = repeat(torch.arange(1, args.d_state + 1), 'n -> d n', d=args.d_inner)\n",
    "        self.A_log = nn.Parameter(torch.log(A))\n",
    "        self.D = nn.Parameter(torch.ones(args.d_inner))\n",
    "        self.out_proj = nn.Linear(args.d_inner, args.d_model, bias=args.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Mamba block forward. This looks the same as Figure 3 in Section 3.4 in the Mamba paper [1].\n",
    "    \n",
    "        Args:\n",
    "            x: shape (b, l, d)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "    \n",
    "        Returns:\n",
    "            output: shape (b, l, d)\n",
    "        \n",
    "        Official Implementation:\n",
    "            class Mamba, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L119\n",
    "            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\n",
    "            \n",
    "        \"\"\"\n",
    "        (b, l, d) = x.shape\n",
    "        \n",
    "        x_and_res = self.in_proj(x)  # shape (b, l, 2 * d_in)\n",
    "        (x, res) = x_and_res.split(split_size=[self.args.d_inner, self.args.d_inner], dim=-1)\n",
    "\n",
    "        x = rearrange(x, 'b l d_in -> b d_in l')\n",
    "        x = self.conv1d(x)[:, :, :l]\n",
    "        x = rearrange(x, 'b d_in l -> b l d_in')\n",
    "        \n",
    "        x = F.silu(x)\n",
    "\n",
    "        y = self.ssm(x)\n",
    "        \n",
    "        y = y * F.silu(res)\n",
    "        \n",
    "        return self.out_proj(y)\n",
    "\n",
    "    def ssm(self, x):\n",
    "        \"\"\"Runs the SSM. See:\n",
    "            - Algorithm 2 in Section 3.2 in the Mamba paper [1]\n",
    "            - run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
    "\n",
    "        Args:\n",
    "            x: shape (b, l, d_in)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "    \n",
    "        Returns:\n",
    "            output: shape (b, l, d_in)\n",
    "\n",
    "        Official Implementation:\n",
    "            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\n",
    "            \n",
    "        \"\"\"\n",
    "        (d_in, n) = self.A_log.shape\n",
    "\n",
    "        # Compute ∆ A B C D, the state space parameters.\n",
    "        #     A, D are input independent (see Mamba paper [1] Section 3.5.2 \"Interpretation of A\" for why A isn't selective)\n",
    "        #     ∆, B, C are input-dependent (this is a key difference between Mamba and the linear time invariant S4,\n",
    "        #                                  and is why Mamba is called **selective** state spaces)\n",
    "        \n",
    "        A = -torch.exp(self.A_log.float())  # shape (d_in, n)\n",
    "        D = self.D.float()\n",
    "\n",
    "        x_dbl = self.x_proj(x)  # (b, l, dt_rank + 2*n)\n",
    "        \n",
    "        (delta, B, C) = x_dbl.split(split_size=[self.args.dt_rank, n, n], dim=-1)  # delta: (b, l, dt_rank). B, C: (b, l, n)\n",
    "        delta = F.softplus(self.dt_proj(delta))  # (b, l, d_in)\n",
    "        \n",
    "        return selective_scan(x, delta, A, B, C, D, mode=self.args.scan_mode)  # This is similar to run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
    "\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
